{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HpfmFgmoXCO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "8ddc44d2-b9e1-4fa3-d6e6-2c792ce0e0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.3.2 in /usr/local/lib/python3.7/dist-packages (4.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (0.0.47)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.2) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "import pandas as pd\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "4bfd4bb9-741f-4cd3-ed17-a3c4ade711ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNw6pEAbNkwO",
        "outputId": "38c760d5-d937-497a-957d-3af0f46189aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhJJYU3uj3QM",
        "outputId": "5a2797d4-03c7-4cea-ef0e-8525eebb51ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgEinXKiXMoP"
      },
      "outputs": [],
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 200\n",
        "batch_size = 32\n",
        "NUM_CLS = 3\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator, \n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1; \n",
        "# number of hidden layers in the discriminator, \n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1; \n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.4\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets, \n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "\n",
        "\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 3e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 20\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = True\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inh8bxClxErv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class Data_Loader():\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,NUM_LABEL = 20,NUM_UNLABEL = 5000,NUM_CLS = 3 ):\n",
        "\n",
        "      self.NUM_LABEL = NUM_LABEL\n",
        "      self.NUM_UNLABEL = NUM_UNLABEL\n",
        "      self.NUM_CLS = NUM_CLS\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def yahoo_data_reader(self):\n",
        "\n",
        "\n",
        "      df =  pd.read_excel(\"/content/drive/MyDrive/IMDB/train.xlsx\")\n",
        "      df_test = pd.read_excel(\"/content/drive/MyDrive/IMDB/test.xlsx\")\n",
        "\n",
        "      df.Sentiment.replace(to_replace=['neg', 'pos'], value=[1, 2], inplace=True)\n",
        "      df_test.Sentiment.replace(to_replace=['neg', 'pos'], value=[1, 2], inplace=True)\n",
        "\n",
        "\n",
        "      df= df.rename(columns={'Sentiment': 'Target','Reviews':'comment'}).iloc[1: , :].reset_index(drop = True)\n",
        "      df_test= df_test.rename(columns={'Sentiment': 'Target','Reviews':'comment'}).iloc[1: , :].reset_index(drop = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      print(df)\n",
        "      print(df_test)\n",
        "\n",
        "      return df,df_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def sample_data( self,data_frame,number = 20,number_of_unlabel = 5000 ,number_of_classes = 2):\n",
        "\n",
        "      df_label = pd.DataFrame({'Target' : [],\n",
        "                              'comment':[]})\n",
        "\n",
        "      df_unlabel = pd.DataFrame({'Target' : [],\n",
        "                              'comment':[]})\n",
        "\n",
        "      num_unlabel_per_class = int(number_of_unlabel/number_of_classes)\n",
        "\n",
        "      for i in range(1,number_of_classes):\n",
        "\n",
        "        df_subset = data_frame[data_frame[\"Target\"] ==i ].sample(n=number,random_state= 42,replace=False)\n",
        "        df_label = df_label.append(df_subset,ignore_index=True)\n",
        "\n",
        "        df_subset1 = data_frame[data_frame[\"Target\"] ==i ].sample(n=num_unlabel_per_class,random_state= 21,replace=False)\n",
        "        df_unlabel = df_unlabel.append(df_subset1,ignore_index=True)\n",
        "\n",
        "\n",
        "      return df_label,df_unlabel\n",
        "\n",
        "\n",
        "\n",
        "    def example_maker(self,dataframe):\n",
        "\n",
        "      ll =[]\n",
        "      for i in range(len(dataframe)):\n",
        "        ll.append((dataframe.comment[i],dataframe.Target[i]))\n",
        "          \n",
        "\n",
        "      return ll  \n",
        "\n",
        "    def example_maker1(self,dataframe):\n",
        "\n",
        "      ll =[]\n",
        "      for i in range(len(dataframe)):\n",
        "        ll.append((dataframe.comment[i],0))\n",
        "          \n",
        "\n",
        "      return ll    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_data_loader(self,input_examples, label_masks, do_shuffle = False, balance_label_examples = False):\n",
        "      '''\n",
        "      Generate a Dataloader given the input examples, eventually masked if they are \n",
        "      to be considered NOT labeled.\n",
        "      '''\n",
        "      examples = []\n",
        "\n",
        "      # Count the percentage of labeled examples  \n",
        "      num_labeled_examples = 0\n",
        "      for label_mask in label_masks:\n",
        "        if label_mask: \n",
        "          num_labeled_examples += 1\n",
        "      label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "      # if required it applies the balance\n",
        "      for index, ex in enumerate(input_examples): \n",
        "        # print(\"index\",index)\n",
        "        # print(\"ex\",ex)\n",
        "\n",
        "        if label_mask_rate == 1 or not balance_label_examples:\n",
        "          examples.append((ex, label_masks[index]))\n",
        "        else:\n",
        "          # IT SIMULATE A LABELED EXAMPLE\n",
        "          if label_masks[index]:\n",
        "            balance = int(1/label_mask_rate)\n",
        "            balance = int(math.log(balance,2))\n",
        "            if balance < 1:\n",
        "              balance = 1\n",
        "            for b in range(0, int(balance)):\n",
        "              examples.append((ex, label_masks[index]))\n",
        "          else:\n",
        "            examples.append((ex, label_masks[index]))\n",
        "\n",
        "      #-----------------------------------------------\n",
        "      # Generate input examples to the Transformer\n",
        "      #-----------------------------------------------\n",
        "      input_ids = []\n",
        "      input_mask_array = []\n",
        "      label_mask_array = []\n",
        "      label_id_array = []\n",
        "\n",
        "      # Tokenization\n",
        "      count = 0 \n",
        "      for (text, label_mask) in examples:\n",
        "        \n",
        "        count = count+1\n",
        "        if(type(text[0]) != int):\n",
        "          encoded_sent = self.tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "          if len(encoded_sent) != 200:\n",
        "            print(len(encoded_sent))\n",
        "          input_ids.append(encoded_sent)\n",
        "          label_id_array.append(text[1])\n",
        "          label_mask_array.append(label_mask)\n",
        "        else:\n",
        "          print(text[0])\n",
        "\n",
        "      # Attention to token (to ignore padded input wordpieces)\n",
        "      for sent in input_ids:\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "        input_mask_array.append(att_mask)\n",
        "      # Convertion to Tensor\n",
        "\n",
        "      input_ids = torch.tensor(input_ids) \n",
        "      input_mask_array = torch.tensor(input_mask_array)\n",
        "      label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "      label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "      # Building the TensorDataset\n",
        "      dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "      if do_shuffle:\n",
        "        sampler = RandomSampler\n",
        "      else:\n",
        "        sampler = SequentialSampler\n",
        "\n",
        "      # Building the DataLoader\n",
        "      return DataLoader(\n",
        "                  dataset,  # The training samples.\n",
        "                  batch_size = batch_size,\n",
        "                  shuffle=True) # Trains with this batch size.\n",
        "\n",
        "    def data_loader(self):\n",
        "\n",
        "      df,test_examples1  = self.yahoo_data_reader()\n",
        "      labeled_examples1,unlabeled_examples1 = self.sample_data(df,number = NUM_LABEL ,number_of_unlabel = 5000,number_of_classes =NUM_CLS )\n",
        "\n",
        "      ########################\n",
        "      labeled_examples = self.example_maker(labeled_examples1)\n",
        "      unlabeled_examples = self.example_maker1(unlabeled_examples1)\n",
        "      test_examples = self.example_maker(test_examples1)\n",
        "      ##########################\n",
        "      #------------------------------\n",
        "      #   Load the test dataset\n",
        "      #------------------------------\n",
        "\n",
        "      train_examples = labeled_examples\n",
        "\n",
        "\n",
        "      train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "\n",
        "      if unlabeled_examples:\n",
        "        train_examples = train_examples + unlabeled_examples\n",
        "        #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "        tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "        train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "      train_dataloader = self.generate_data_loader(train_examples, train_label_masks, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "      #The labeled (test) dataset is assigned with a mask set to True\n",
        "      test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "      test_dataloader = self.generate_data_loader(test_examples, test_label_masks, do_shuffle = False, balance_label_examples = False)\n",
        "      ####################################################\n",
        "\n",
        "      return train_dataloader,test_dataloader,train_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qysfVLoeS5MF"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ewsoFSW_gI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#------------------------------\n",
        "#   The Generator as in \n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "        #hidden_sizes[-1] = 512\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        print(\"G:layers\",layers)\n",
        "        print(\"G:self.layers\", self.layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.attr = nn.Linear(768 + 3,768 )\n",
        "\n",
        "        self.relu= nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        # print(\"D:layers\",layers)\n",
        "        # print(\"D:self.layers\", self.layers)\n",
        "        # print(\"self.logit\",self.logit)\n",
        "\n",
        "\n",
        "    def forward(self, input_rep, label = None,label_embed = None):\n",
        "\n",
        "\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "\n",
        "        return last_rep, logits, probs\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def noise_gen(input_shape, noise_size, device):\n",
        "\n",
        "  return torch.zeros(input_shape,noise_size, device=device).uniform_(0, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(transformer,generator,discriminator,train_iter,gen_optimizer,dis_optimizer,scheduler_d,scheduler_g):\n",
        "\n",
        "  transformer.train() \n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  # Reset the total loss for this epoch.\n",
        "  tr_g_loss = 0\n",
        "  tr_d_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(train_iter):\n",
        "\n",
        "\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      b_label_mask = batch[3].to(device)\n",
        "\n",
        "      # label_embed = generator.label_embedding\n",
        "\n",
        "\n",
        "      model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "      hidden_states = model_outputs[-1]\n",
        "\n",
        "      #NDA = hidden_states.detach()\n",
        "      NDA = hidden_states\n",
        "      #add label stuff\n",
        "      real_feature, real_logits, real_probs = discriminator(hidden_states)\n",
        "\n",
        "\n",
        "      #---------------------------------\n",
        "      # Simple Generator\n",
        "      #---------------------------------\n",
        "\n",
        "      \n",
        "      #fake data the same batch size of unlabel data\n",
        "      noise = noise_gen(b_input_ids.shape[0], noise_size, device)\n",
        "      \n",
        "      # Gnerate Fake data\n",
        "      gen_rep = generator(noise)\n",
        "\n",
        "      alpha = 0.9 \n",
        "      l = np.random.beta(alpha, alpha)\n",
        "      l = max(l, 1-l)\n",
        "      #l= 0.9\n",
        "      l= 0.85\n",
        "      neg_aug = l * gen_rep + (1 - l) * NDA\n",
        "      neg_aug = neg_aug.to(device)\n",
        "      fake_feature, fake_logits, fake_probs  = discriminator(neg_aug)\n",
        "\n",
        "\n",
        "\n",
        "      #---------------------------------\n",
        "      # Generator's LOSS estimation\n",
        "      #---------------------------------\n",
        "      g_loss_d =  -1 * torch.mean(torch.log(1 - fake_probs[:, -1] + epsilon))\n",
        "      \n",
        "      g_feat_reg = torch.mean(torch.pow(torch.mean(real_feature, dim=0) - torch.mean(fake_feature, dim=0), 2))\n",
        "      g_loss = g_loss_d + g_feat_reg \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #---------------------------------\n",
        "      # Discriminator's LOSS estimation\n",
        "      #---------------------------------\n",
        "      D_L_Supervised = supervised_loss(real_logits,b_labels,b_label_mask)\n",
        "      D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - real_probs[:, -1] + epsilon))\n",
        "      D_L_unsupervised2U = -1 * torch.mean(torch.log(fake_probs[:, -1] + epsilon))\n",
        "      # D_L_unsupervised2U_simple = -1 * torch.mean(torch.log(D_fake_probs1[:, -1] + epsilon))\n",
        "      d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "\n",
        "      #---------------------------------\n",
        "      #  OPTIMIZATION\n",
        "      #---------------------------------\n",
        "      # Avoid gradient accumulation\n",
        "      gen_optimizer.zero_grad()\n",
        "      dis_optimizer.zero_grad()\n",
        "\n",
        "      # Calculate weigth updates\n",
        "      # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "      g_loss.backward(retain_graph=True)\n",
        "      # d_loss.backward() \n",
        "      d_loss.backward()\n",
        "      \n",
        "      # Apply modifications\n",
        "      gen_optimizer.step()\n",
        "      dis_optimizer.step()\n",
        "\n",
        "      # Save the losses to print them later\n",
        "      tr_g_loss += g_loss.item()\n",
        "      tr_d_loss += d_loss.item()\n",
        "\n",
        "\n",
        "      # Update the learning rate with the scheduler\n",
        "      if apply_scheduler:\n",
        "        scheduler_d.step()\n",
        "        scheduler_g.step()\n",
        "\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  ###### it is very important how many times we use label data and how we calculate the loss\n",
        "  avg_train_loss_g = tr_g_loss / len(train_iter)\n",
        "  avg_train_loss_d = tr_d_loss/ len(train_iter)             \n",
        "\n",
        "  # Measure how long this epoch took.\n",
        "  training_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
        "  print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "\n",
        "  print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "  result_dic = {\n",
        "      \n",
        "      'Training Loss generator': avg_train_loss_g,\n",
        "      'Training Loss discriminator sup': avg_train_loss_d,\n",
        "      'Training Time': training_time,\n",
        "  }\n",
        "\n",
        "  return result_dic\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def supervised_loss(D_real_logits,b_labels,b_label_mask):\n",
        "\n",
        "  # Disciminator's LOSS estimation\n",
        "  logits = D_real_logits[:,0:-1]\n",
        "  # print(\"logits\",logits.shape)\n",
        "  log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "  label2one_hot = torch.nn.functional.one_hot(b_labels, NUM_CLS)\n",
        "  \n",
        "  per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "\n",
        "  per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "  \n",
        "  labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "  \n",
        "  if labeled_example_count == 0:\n",
        "    D_L_Supervised = 0\n",
        "  else:\n",
        "    D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "\n",
        "  return D_L_Supervised\n",
        "\n",
        "\n",
        "#loss\n",
        "nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "def valid(transformer,generator,discriminator,test_dataloader,nll_loss):\n",
        "\n",
        "  print(\"Running Test...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        " \n",
        "  transformer.eval() #maybe redundant\n",
        "  discriminator.eval()\n",
        "  generator.eval()\n",
        "\n",
        "\n",
        "  # Tracking variables \n",
        "  total_test_accuracy = 0\n",
        "  total_test_loss = 0\n",
        "  nb_test_steps = 0\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels_ids = []\n",
        "  accurcies = []\n",
        "\n",
        "  #loss\n",
        "  nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in test_dataloader:\n",
        "      \n",
        "      # Unpack this training batch from our dataloader. \n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      \n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():        \n",
        "          model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "          hidden_states = model_outputs[-1]\n",
        "          _, logits, probs = discriminator(hidden_states)\n",
        "          filtered_logits = logits[:,0:-1]\n",
        "          #print(\"filtered_logits\",filtered_logits)\n",
        "          total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "          \n",
        "      # Accumulate the predictions and the input labels\n",
        "      _, preds = torch.max(filtered_logits, 1)\n",
        "      # print(\"preds\",preds)\n",
        "      # print(\"b_labels\",b_labels)\n",
        "\n",
        "      all_preds += preds.detach().cpu()\n",
        "      all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  all_preds = torch.stack(all_preds).numpy()\n",
        "  all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "  test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "  accurcies.append(test_accuracy)\n",
        "  print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "  F1 = f1_score(all_labels_ids, all_preds,average=\"weighted\")\n",
        "  print(\"F1: \",F1)\n",
        "\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "  avg_test_loss = avg_test_loss.item()\n",
        "  \n",
        "  # Measure how long the validation run took.\n",
        "  test_time = format_time(time.time() - t0)\n",
        "  \n",
        "  print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "  print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "  return test_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCc9Pcaghcof"
      },
      "outputs": [],
      "source": [
        "# #number_of_labeled_data for each class\n",
        "# NUM_LABEL = 20\n",
        "\n",
        "# #number_of_unlabeled_data \n",
        "# NUM_UNLABEL = 5000\n",
        "\n",
        "# #number of classes +1 \n",
        "# NUM_CLS = 3\n",
        "# train_dataloader,test_dataloader,train_examples = Data_Loader(NUM_LABEL,NUM_UNLABEL ,NUM_CLS).data_loader()\n",
        "\n",
        "\n",
        "\n",
        "# # The config file is required to get the dimension of the vector produced by \n",
        "# # the underlying transformer\n",
        "# config = AutoConfig.from_pretrained(model_name)\n",
        "# hidden_size = int(config.hidden_size)\n",
        "\n",
        "# # Define the number and width of hidden layers\n",
        "# hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "# hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "# # print(\"hidden_levels_g\",hidden_levels_g)\n",
        "# # print(\"hidden_levels_d\",hidden_levels_d)\n",
        "\n",
        "# #-------------------------------------------------\n",
        "# #   Instantiate the Generator and Discriminator\n",
        "# #-------------------------------------------------\n",
        "\n",
        "# generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)\n",
        "\n",
        "# discriminator = Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS , dropout_rate=out_dropout_rate)\n",
        "\n",
        "# # Put everything in the GPU if available\n",
        "# if torch.cuda.is_available():    \n",
        "#   generator.cuda()\n",
        "#   discriminator.cuda()\n",
        "#   transformer.cuda()\n",
        "\n",
        "#   if multi_gpu:\n",
        "#     transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "\n",
        "# # Measure the total training time for the whole run.\n",
        "# total_t0 = time.time()\n",
        "\n",
        "# #models parameters\n",
        "# transformer_vars = [i for i in transformer.parameters()]\n",
        "# d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "# g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "# #optimizer\n",
        "# dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "# gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "\n",
        "# #scheduler\n",
        "# if apply_scheduler:\n",
        "#   num_train_examples = len(train_examples)\n",
        "#   num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "#   num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "#   scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "#                                            num_warmup_steps = num_warmup_steps)\n",
        "#   scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "#                                            num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "\n",
        "# # PATH =\"/content/drive/MyDrive/20news/NDABERT\"\n",
        "\n",
        "# PATH =\"/content/drive/MyDrive/ex2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Zi8innV-EG"
      },
      "outputs": [],
      "source": [
        "# max_accuracy = 0\n",
        "# for epoch_i in range(0, 15):\n",
        "\n",
        "#   train(transformer,generator,discriminator,train_dataloader,gen_optimizer,dis_optimizer,scheduler_d,scheduler_g)\n",
        "#   test_accuracy = valid(transformer,generator,discriminator,test_dataloader,nll_loss)\n",
        "#   if test_accuracy >= max_accuracy:\n",
        "#     max_accuracy = test_accuracy\n",
        "#     torch.save(generator, PATH +\"NDA_imdb_generator_20.pt\")\n",
        "#     torch.save(discriminator, PATH +\"NDA_imdb_discriminator_20.pt\")\n",
        "#     torch.save(transformer, PATH +\"NDA_imdb_transformer_20.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCQz9RF5tbqe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50wAyyOLmZX",
        "outputId": "287780fc-81be-4142-fd1b-0569c25fde83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 comment  Target\n",
            "0      Mere thoughts of \"Going Overboard\" (aka \"Babes...       1\n",
            "1      Why does this movie fall WELL below standards?...       1\n",
            "2      Wow and I thought that any Steven Segal movie ...       1\n",
            "3      The story is seen before, but that does'n matt...       1\n",
            "4      Like so many media experiments, this amateuris...       1\n",
            "...                                                  ...     ...\n",
            "24994  Everyone plays their part pretty well in this ...       2\n",
            "24995  It happened with Assault on Prescient 13 in 20...       1\n",
            "24996  My God. This movie was awful. I can't complain...       1\n",
            "24997  When I first popped in Happy Birthday to Me, I...       1\n",
            "24998  So why does this show suck? Unfortunately, tha...       1\n",
            "\n",
            "[24999 rows x 2 columns]\n",
            "                                                 comment  Target\n",
            "0      After realizing what is going on around us ......       2\n",
            "1      I grew up watching the original Disney Cindere...       1\n",
            "2      David Mamet wrote the screenplay and made his ...       2\n",
            "3      Admittedly, I didn't have high expectations of...       1\n",
            "4      OK - you want to test somebody on how comforta...       2\n",
            "...                                                  ...     ...\n",
            "24994  This fanciful horror flick has Vincent Price p...       1\n",
            "24995  The Intruder (L'Intrus), a film directed by Fr...       2\n",
            "24996  Holy crap. This was the worst film I have seen...       1\n",
            "24997  Clocking in at an interminable three hours and...       1\n",
            "24998  Rented and watched this short (< 90 minutes) w...       2\n",
            "\n",
            "[24999 rows x 2 columns]\n",
            "0\n",
            "G:layers [Linear(in_features=100, out_features=768, bias=True), LeakyReLU(negative_slope=0.2, inplace=True), Dropout(p=0.4, inplace=False), Linear(in_features=768, out_features=768, bias=True)]\n",
            "G:self.layers Sequential(\n",
            "  (0): Linear(in_features=100, out_features=768, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  (2): Dropout(p=0.4, inplace=False)\n",
            "  (3): Linear(in_features=768, out_features=768, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#number_of_labeled_data for each class\n",
        "NUM_LABEL = 1000\n",
        "\n",
        "#number_of_unlabeled_data \n",
        "NUM_UNLABEL = 5000\n",
        "\n",
        "#number of classes +1 \n",
        "NUM_CLS = 3\n",
        "train_dataloader,test_dataloader,train_examples = Data_Loader(NUM_LABEL,NUM_UNLABEL ,NUM_CLS).data_loader()\n",
        "\n",
        "\n",
        "\n",
        "# The config file is required to get the dimension of the vector produced by \n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "# print(\"hidden_levels_g\",hidden_levels_g)\n",
        "# print(\"hidden_levels_d\",hidden_levels_d)\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=[768], dropout_rate=out_dropout_rate)\n",
        "\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=[768], num_labels=NUM_CLS , dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "\n",
        "#scheduler\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "\n",
        "# PATH =\"/content/drive/MyDrive/20news/NDABERT\"\n",
        "\n",
        "PATH =\"/content/drive/MyDrive/ex2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L0WB6pv3EtU",
        "outputId": "8291f5b4-f6c6-49c0-cd49-6c9f68880f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average training loss generetor: 0.464\n",
            "  Average training loss discriminator: 2.092\n",
            "  Training epcoh took: 0:01:31\n",
            "Running Test...\n",
            "  Accuracy: 0.824\n",
            "F1:  0.821636075273582\n",
            "  Test Loss: 0.417\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.738\n",
            "  Average training loss discriminator: 1.120\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.853\n",
            "F1:  0.8523697847987189\n",
            "  Test Loss: 0.391\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.722\n",
            "  Average training loss discriminator: 0.983\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.863\n",
            "F1:  0.8626892408079944\n",
            "  Test Loss: 0.372\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.714\n",
            "  Average training loss discriminator: 0.843\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.846\n",
            "F1:  0.844877544347417\n",
            "  Test Loss: 0.401\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.712\n",
            "  Average training loss discriminator: 0.827\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.872\n",
            "F1:  0.8720654015967977\n",
            "  Test Loss: 0.420\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.710\n",
            "  Average training loss discriminator: 0.782\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.857\n",
            "F1:  0.8565877814487931\n",
            "  Test Loss: 0.593\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.709\n",
            "  Average training loss discriminator: 0.761\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.842\n",
            "F1:  0.8418079484256444\n",
            "  Test Loss: 0.524\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.706\n",
            "  Average training loss discriminator: 0.760\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.835\n",
            "F1:  0.8335934593717415\n",
            "  Test Loss: 0.726\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.707\n",
            "  Average training loss discriminator: 0.760\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.852\n",
            "F1:  0.8517609545632864\n",
            "  Test Loss: 0.656\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.706\n",
            "  Average training loss discriminator: 0.753\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.862\n",
            "F1:  0.8616687342857832\n",
            "  Test Loss: 0.681\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.704\n",
            "  Average training loss discriminator: 0.727\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.848\n",
            "F1:  0.8483955530548464\n",
            "  Test Loss: 0.757\n",
            "  Test took: 0:01:28\n",
            "\n",
            "  Average training loss generetor: 0.705\n",
            "  Average training loss discriminator: 0.762\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.838\n",
            "F1:  0.8360202373721162\n",
            "  Test Loss: 0.773\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.705\n",
            "  Average training loss discriminator: 0.744\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.860\n",
            "F1:  0.8595021485697333\n",
            "  Test Loss: 0.787\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.704\n",
            "  Average training loss discriminator: 0.716\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.860\n",
            "F1:  0.8604187004230193\n",
            "  Test Loss: 0.869\n",
            "  Test took: 0:01:27\n",
            "\n",
            "  Average training loss generetor: 0.707\n",
            "  Average training loss discriminator: 0.785\n",
            "  Training epcoh took: 0:01:32\n",
            "Running Test...\n",
            "  Accuracy: 0.854\n",
            "F1:  0.8534707631752679\n",
            "  Test Loss: 0.847\n",
            "  Test took: 0:01:27\n"
          ]
        }
      ],
      "source": [
        "max_accuracy = 0\n",
        "for epoch_i in range(0, 15):\n",
        "\n",
        "  train(transformer,generator,discriminator,train_dataloader,gen_optimizer,dis_optimizer,scheduler_d,scheduler_g)\n",
        "  test_accuracy = valid(transformer,generator,discriminator,test_dataloader,nll_loss)\n",
        "  if test_accuracy >= max_accuracy:\n",
        "    max_accuracy = test_accuracy\n",
        "    torch.save(generator, PATH +\"NDA_imdb_generator_1000.pt\")\n",
        "    torch.save(discriminator, PATH +\"NDA_imdb_discriminator_1000.pt\")\n",
        "    torch.save(transformer, PATH +\"NDA_imdb_transformer_1000.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "(1Flairs_IMDB)NDA_GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}